{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Evaluation\n",
    "## Table of Contents\n",
    "1. [Model Selection](#model-selection)\n",
    "2. [Model improvement 1](#model-improvement-1)\n",
    "3. [Model improvement 2](#model-improvement-2)\n",
    "4. [Making predictions](#making-predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Following adjustments were tried and tested as part of building a more robust model architechture, over the previous simple baseline model:\n",
    "\n",
    "1. Adjusting the learning rate of the optimizer\n",
    "2. Adding more hidden layers in the neural network, using ReLU activation function (Model 1)\n",
    "3. Adding dropout layers (Model 2)\n",
    "\n",
    "Finally model 2 (trained on maxprob scenario) was used to make predictions of Species richness in the other 2 climatic scenarios (mext and median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model improvement 1\n",
    "### By adding more hidden layers\n",
    "\n",
    "Increased Model Capacity:\n",
    "- By adding more layers increases its capacity to capture complex patterns and relationships in the data. A more complex model can better represent the underlying structure of your regression problem.\n",
    "\n",
    "\n",
    "Non Linearity and Improved Representation:\n",
    "- Introducing ReLU activation functions between layers allows the model to learn non linear mappings, leading to better approximation of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 1\n",
    "# Define R-squared as a custom metric\n",
    "def r_squared(y_true, y_pred):\n",
    "    SS_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    SS_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    return 1 - (SS_res / (SS_tot + tf.keras.backend.epsilon()))\n",
    "\n",
    "# model 1\n",
    "m1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(3,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # Output layer with one neuron for species richness prediction\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "m1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error', metrics=[r_squared])\n",
    "\n",
    "# Train the model\n",
    "history = m1.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "\n",
    "# To check & visualize if the model is overfitting\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the baseline model on test data\n",
    "loss, mae = m1.evaluate(X_test, y_test, verbose=0)\n",
    "#loss, mae = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Predictions from the baseline model (already obtained)\n",
    "predictions_m1 = m1.predict(X_test)\n",
    "\n",
    "\n",
    "r2 = r2_score(y_test, predictions_m1)\n",
    "print(f' Model 1 R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model improvement 2\n",
    "### By adding dropout layers\n",
    "\n",
    "- Dropout is a regularization technique that randomly drops (sets to zero) a fraction of input units during training.\n",
    "- Helps prevent co adaptation of neurons and encourages the network to learn more robust features. By randomly dropping units during training, dropout introduces noise and reduces the risk of overfitting to the training data .\n",
    "- Dropout is often more beneficial when you have a limited amount of training data.\n",
    "\n",
    "\n",
    "- Dropout tends to be more effective in larger and more complex models .\n",
    "- As the regression model is relatively simple, the regularization provided by\n",
    "dropout did not have a substantial impact on the model accuracy at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2 With dropiut layers\n",
    "m2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(3,)),\n",
    "    tf.keras.layers.Dropout(0.2),  # Add dropout with a dropout rate (e.g., 0.2)\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # Output layer with one neuron for species richness prediction\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "m2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error', metrics=[r_squared])\n",
    "\n",
    "# Train the model\n",
    "history = m2.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "\n",
    "# TO check & visualize if the model is overfitting\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the baseline model on test data\n",
    "loss, mae = m2.evaluate(X_test, y_test, verbose=0)\n",
    "#loss, mae = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Predictions from the baseline model (already obtained)\n",
    "predictions_m1 = m2.predict(X_test)\n",
    "\n",
    "\n",
    "r2 = r2_score(y_test, predictions_m1)\n",
    "print(f' Model 2 R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "Model 2 ('m2') was selected finally, out of the 3 climate scenarios, this model was trained using the data of maxprob scenario.\n",
    "\n",
    "This 'm2' is now used to predict the target (values of species richness) in the mext and median climatic scenarios. And the values was then plotted in a trend plot along with the maxprob scenarios.\n",
    "\n",
    "This plot shows all the 3 SR values in all 3 different climatic scenarios for further comparitive anaysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions for median and mext dataset\n",
    "\n",
    "# Ensure that the columns match the features used during training\n",
    "features_median = median[['Age', 'Mean_Temperature', 'elevation']]\n",
    "features_mext = mext[['Age', 'Mean_Temperature', 'elevation']]\n",
    "\n",
    "# Standardize the features using the same scaler used during training\n",
    "scaler = StandardScaler()\n",
    "features_median_scaled = scaler.fit_transform(features_median)\n",
    "features_mext_scaled = scaler.transform(features_mext)\n",
    "\n",
    "# Use the trained model to predict species richness for each scenario\n",
    "predictions_median = m1.predict(features_median_scaled)\n",
    "predictions_mext = m1.predict(features_mext_scaled)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot actual values from max probability scenario\n",
    "plt.plot(-maxprob['Age'], maxprob['SR_total'], label='Max Probability (Blue)', color='blue')\n",
    "\n",
    "# Plot predictions for mass extinction scenario\n",
    "plt.plot(-mext['Age'], predictions_mext, label='Median Climate (Green)', color='Green')\n",
    "\n",
    "# Plot predictions for median scenario\n",
    "plt.plot(-median['Age'], predictions_median, label='Mass Extictions (Red)', color='red')\n",
    "\n",
    "\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Age (myrs)')\n",
    "plt.ylabel('Species Richness')\n",
    "plt.title('Predicted Species Richness in Different Climate Scenarios')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
